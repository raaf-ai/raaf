name: ðŸŒ™ DSL Nightly - Extended Testing & Feature Validation

on:
  schedule:
    # Run every night at 3 AM UTC (different from core and providers to spread load)
    - cron: '0 3 * * *'
  workflow_dispatch:
    inputs:
      full_integration:
        description: 'Run full integration tests with real API calls'
        required: false
        default: 'false'
        type: boolean

env:
  RUBY_VERSION: '3.4.5'

jobs:
  # Extended compatibility matrix
  compatibility-matrix:
    name: "ðŸ”„ Ruby ${{ matrix.ruby-version }} / ${{ matrix.os }} Compatibility"
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        ruby-version: ['3.1', '3.2', '3.3', '3.4.5']
        os: [ubuntu-latest, macos-latest, windows-latest]
        exclude:
          # Ruby 3.1 has issues on Windows with some gems
          - ruby-version: '3.1'
            os: windows-latest
    
    steps:
    - uses: actions/checkout@v4

    - name: Set up Ruby ${{ matrix.ruby-version }}
      uses: ruby/setup-ruby@v1
      with:
        ruby-version: ${{ matrix.ruby-version }}
        bundler-cache: true

    - name: Install dependencies (DSL gem)
      run: |
        cd dsl
        bundle install

    - name: Run core tests
      run: |
        cd dsl
        bundle exec rspec --tag ~integration --tag ~performance --tag ~acceptance \
                         --format progress
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY_TEST }}
        TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY_TEST }}

    - name: Build and validate gem
      run: |
        cd dsl
        gem build raaf-dsl.gemspec
        gem install --local raaf-dsl-*.gem
        ruby -r raaf-dsl -e "puts 'DSL gem loaded on ${{ matrix.ruby-version }}/${{ matrix.os }}'"

  # DSL feature matrix testing
  dsl-feature-matrix:
    name: "ðŸŽ¨ DSL Feature: ${{ matrix.feature }}"
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        feature: 
          - agent_builder
          - workflow_builder
          - prompt_resolution
          - debug_tools
          - tool_dsl
          - web_search
    
    steps:
    - uses: actions/checkout@v4

    - name: Set up Ruby
      uses: ruby/setup-ruby@v1
      with:
        ruby-version: ${{ env.RUBY_VERSION }}
        bundler-cache: true

    - name: Install dependencies (DSL gem)
      run: |
        cd dsl
        bundle install

    - name: Test ${{ matrix.feature }} feature
      run: |
        cd dsl
        bundle exec rspec --tag ${{ matrix.feature }} \
                         --format progress \
                         --format RspecJunitFormatter \
                         --out test-results-${{ matrix.feature }}.xml
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY_TEST }}
        TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY_TEST }}

    - name: Upload ${{ matrix.feature }} test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: ${{ matrix.feature }}-feature-test-results
        path: dsl/test-results-${{ matrix.feature }}.xml
        retention-days: 14

  # Prompt resolution comprehensive testing
  prompt-resolution-extended:
    name: "ðŸ“ Extended Prompt Resolution"
    runs-on: ubuntu-latest
    if: github.event.inputs.full_integration == 'true' || github.event_name == 'schedule'
    
    steps:
    - uses: actions/checkout@v4

    - name: Set up Ruby
      uses: ruby/setup-ruby@v1
      with:
        ruby-version: ${{ env.RUBY_VERSION }}
        bundler-cache: true

    - name: Install dependencies (DSL gem)
      run: |
        cd dsl
        bundle install

    - name: Test prompt resolution scenarios
      run: |
        cd dsl
        bundle exec rspec --tag prompt_extended \
                         --format progress \
                         --format RspecJunitFormatter \
                         --out test-results-prompt-extended.xml
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY_NIGHTLY }}
        TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY_NIGHTLY }}

    - name: Upload extended prompt test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: prompt-extended-test-results
        path: dsl/test-results-prompt-extended.xml
        retention-days: 14

  # Performance benchmarks
  performance-benchmarks:
    name: "âš¡ Performance Benchmarks"
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4

    - name: Set up Ruby
      uses: ruby/setup-ruby@v1
      with:
        ruby-version: ${{ env.RUBY_VERSION }}
        bundler-cache: true

    - name: Install dependencies (DSL gem)
      run: |
        cd dsl
        bundle install

    - name: Run performance benchmarks
      run: |
        cd dsl
        bundle exec rspec --tag benchmark \
                         --format progress \
                         --format RspecJunitFormatter \
                         --out test-results-benchmarks.xml
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY_TEST }}
        TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY_TEST }}

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: benchmark-results
        path: dsl/test-results-benchmarks.xml
        retention-days: 30

    - name: Generate performance report
      run: |
        cd dsl
        # Generate a simple performance report
        echo "## DSL Performance Benchmarks" > performance_report.md
        echo "" >> performance_report.md
        echo "Generated on: $(date)" >> performance_report.md
        echo "" >> performance_report.md
        
        # Add any specific performance metrics here
        echo "### Key Metrics" >> performance_report.md
        echo "- Agent builder performance tracked" >> performance_report.md
        echo "- Prompt resolution timing measured" >> performance_report.md
        echo "- DSL parsing performance analyzed" >> performance_report.md
        echo "- Memory usage for large workflows tracked" >> performance_report.md

    - name: Upload performance report
      uses: actions/upload-artifact@v4
      with:
        name: performance-report
        path: dsl/performance_report.md
        retention-days: 30

  # DSL integration scenarios
  dsl-integration-scenarios:
    name: "ðŸ”— DSL Integration Scenarios"
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4

    - name: Set up Ruby
      uses: ruby/setup-ruby@v1
      with:
        ruby-version: ${{ env.RUBY_VERSION }}
        bundler-cache: true

    - name: Install dependencies (DSL gem)
      run: |
        cd dsl
        bundle install

    - name: Test complex DSL scenarios
      run: |
        cd dsl
        bundle exec rspec --tag integration_scenarios \
                         --format progress \
                         --format RspecJunitFormatter \
                         --out test-results-integration-scenarios.xml
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY_TEST }}
        TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY_TEST }}

    - name: Upload integration scenario test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: integration-scenario-test-results
        path: dsl/test-results-integration-scenarios.xml
        retention-days: 14

  # Security and dependency audit
  security-audit:
    name: "ðŸ”’ Security Audit"
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4

    - name: Set up Ruby
      uses: ruby/setup-ruby@v1
      with:
        ruby-version: ${{ env.RUBY_VERSION }}
        bundler-cache: true

    - name: Install dependencies (DSL gem)
      run: |
        cd dsl
        bundle install

    - name: Run bundle audit
      run: |
        cd dsl
        gem install bundler-audit
        bundle audit check --update
        bundle audit check --format json --output audit-report.json || true

    - name: Check for security vulnerabilities
      run: |
        cd dsl
        # Additional security checks for DSL
        echo "Running DSL-specific security checks..."
        
        # Check for hardcoded API keys in examples
        if grep -r "sk-" examples/ 2>/dev/null | grep -v "your-api-key" | grep -v "test-key"; then
          echo "âš ï¸ Found potential hardcoded API keys in examples"
          exit 1
        fi
        
        # Check for eval usage in DSL files (potential code injection)
        if grep -r "eval\|instance_eval\|class_eval" lib/ --include="*.rb" | grep -v "# eval:" | grep -v "# safe eval"; then
          echo "âš ï¸ Found potential unsafe eval usage in DSL files"
          echo "Please review these usages for security implications"
          # Don't exit 1 here as some eval usage might be intentional in DSL
        fi
        
        # Check for sensitive data in DSL files
        if grep -r "password\|secret\|private_key" lib/ --include="*.rb" | grep -v "API_KEY" | grep -v "password:" | grep -v "secret:"; then
          echo "âš ï¸ Found potential sensitive data in DSL files"
          exit 1
        fi
        
        echo "âœ… Security checks passed"

    - name: Upload audit report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-audit-report
        path: dsl/audit-report.json
        retention-days: 30

  # Documentation and example validation
  docs-validation:
    name: "ðŸ“š Documentation Validation"
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4

    - name: Set up Ruby
      uses: ruby/setup-ruby@v1
      with:
        ruby-version: ${{ env.RUBY_VERSION }}
        bundler-cache: true

    - name: Install dependencies (DSL gem)
      run: |
        cd dsl
        bundle install

    - name: Validate DSL examples
      run: |
        cd dsl
        # Test that DSL examples can be loaded and validated
        for example in examples/*_example.rb; do
          if [ -f "$example" ]; then
            echo "Validating $example..."
            ruby -c "$example" || exit 1
          fi
        done
        echo "All DSL examples validated successfully"
      env:
        CI: true
        RAAF_TEST_MODE: true

    - name: Generate documentation
      run: |
        cd dsl
        # Generate YARD documentation
        bundle exec yard doc --fail-on-warning
        echo "Documentation generated successfully"

    - name: Upload documentation validation report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: docs-validation-report
        path: dsl/doc/
        retention-days: 7

  # Nightly summary
  nightly-summary:
    name: "ðŸ“Š Nightly Summary"
    runs-on: ubuntu-latest
    needs: [compatibility-matrix, dsl-feature-matrix, prompt-resolution-extended, performance-benchmarks, dsl-integration-scenarios, security-audit, docs-validation]
    if: always()
    
    steps:
    - name: Generate nightly summary
      run: |
        echo "## ðŸŒ™ DSL Nightly Test Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Date:** $(date)" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Compatibility results
        echo "### ðŸ”„ Compatibility Matrix" >> $GITHUB_STEP_SUMMARY
        if [[ "${{ needs.compatibility-matrix.result }}" == "success" ]]; then
          echo "âœ… **Cross-platform compatibility**: PASSED" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **Cross-platform compatibility**: FAILED" >> $GITHUB_STEP_SUMMARY
        fi
        
        # DSL feature results
        echo "### ðŸŽ¨ DSL Feature Matrix" >> $GITHUB_STEP_SUMMARY
        if [[ "${{ needs.dsl-feature-matrix.result }}" == "success" ]]; then
          echo "âœ… **DSL feature tests**: PASSED" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **DSL feature tests**: FAILED" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Prompt resolution results
        echo "### ðŸ“ Extended Prompt Resolution" >> $GITHUB_STEP_SUMMARY
        if [[ "${{ needs.prompt-resolution-extended.result }}" == "success" ]]; then
          echo "âœ… **Extended prompt resolution**: PASSED" >> $GITHUB_STEP_SUMMARY
        elif [[ "${{ needs.prompt-resolution-extended.result }}" == "skipped" ]]; then
          echo "â­ï¸ **Extended prompt resolution**: SKIPPED (no full integration)" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **Extended prompt resolution**: FAILED" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Performance results
        echo "### âš¡ Performance Benchmarks" >> $GITHUB_STEP_SUMMARY
        if [[ "${{ needs.performance-benchmarks.result }}" == "success" ]]; then
          echo "âœ… **Performance benchmarks**: PASSED" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **Performance benchmarks**: FAILED" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Integration scenarios results
        echo "### ðŸ”— DSL Integration Scenarios" >> $GITHUB_STEP_SUMMARY
        if [[ "${{ needs.dsl-integration-scenarios.result }}" == "success" ]]; then
          echo "âœ… **DSL integration scenarios**: PASSED" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **DSL integration scenarios**: FAILED" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Security results
        echo "### ðŸ”’ Security Audit" >> $GITHUB_STEP_SUMMARY
        if [[ "${{ needs.security-audit.result }}" == "success" ]]; then
          echo "âœ… **Security audit**: PASSED" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **Security audit**: FAILED" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Documentation results
        echo "### ðŸ“š Documentation Validation" >> $GITHUB_STEP_SUMMARY
        if [[ "${{ needs.docs-validation.result }}" == "success" ]]; then
          echo "âœ… **Documentation validation**: PASSED" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **Documentation validation**: FAILED" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "ðŸ“‹ **Test Categories:**" >> $GITHUB_STEP_SUMMARY
        echo "- Cross-platform compatibility (Ruby 3.1-3.4, Ubuntu/macOS/Windows)" >> $GITHUB_STEP_SUMMARY
        echo "- DSL feature matrix testing (AgentBuilder, WorkflowBuilder, etc.)" >> $GITHUB_STEP_SUMMARY
        echo "- Extended prompt resolution scenarios" >> $GITHUB_STEP_SUMMARY
        echo "- Performance benchmarks and memory usage" >> $GITHUB_STEP_SUMMARY
        echo "- Complex DSL integration scenarios" >> $GITHUB_STEP_SUMMARY
        echo "- Security audit and vulnerability scanning" >> $GITHUB_STEP_SUMMARY
        echo "- Documentation and example validation" >> $GITHUB_STEP_SUMMARY

    - name: Create issue on failure
      if: needs.compatibility-matrix.result == 'failure' || needs.dsl-feature-matrix.result == 'failure' || needs.prompt-resolution-extended.result == 'failure' || needs.performance-benchmarks.result == 'failure' || needs.dsl-integration-scenarios.result == 'failure' || needs.security-audit.result == 'failure' || needs.docs-validation.result == 'failure'
      uses: actions/github-script@v7
      with:
        script: |
          const title = `ðŸŒ™ DSL Nightly Tests Failed - ${new Date().toISOString().split('T')[0]}`;
          const body = `## Nightly Test Failure Report
          
          The nightly DSL test suite has failed. Please investigate the following:
          
          ### Failed Jobs
          - **Compatibility Matrix**: ${{ needs.compatibility-matrix.result }}
          - **DSL Feature Matrix**: ${{ needs.dsl-feature-matrix.result }}
          - **Extended Prompt Resolution**: ${{ needs.prompt-resolution-extended.result }}
          - **Performance Benchmarks**: ${{ needs.performance-benchmarks.result }}
          - **DSL Integration Scenarios**: ${{ needs.dsl-integration-scenarios.result }}
          - **Security Audit**: ${{ needs.security-audit.result }}
          - **Documentation Validation**: ${{ needs.docs-validation.result }}
          
          ### Action Required
          1. Check the [workflow run](${context.payload.repository.html_url}/actions/runs/${context.runId}) for detailed logs
          2. Review any failing DSL features or compatibility issues
          3. Address security vulnerabilities if found
          4. Update DSL implementations if needed
          5. Fix documentation issues if found
          
          ### Workflow Information
          - **Trigger**: ${context.eventName}
          - **Branch**: ${context.ref}
          - **Commit**: ${context.sha}
          - **Date**: ${new Date().toISOString()}
          
          /cc @team-leads`;
          
          // Check if similar issue already exists
          const issues = await github.rest.issues.listForRepo({
            owner: context.repo.owner,
            repo: context.repo.repo,
            labels: ['nightly-failure', 'dsl'],
            state: 'open'
          });
          
          if (issues.data.length === 0) {
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: title,
              body: body,
              labels: ['nightly-failure', 'dsl', 'investigation-needed']
            });
          } else {
            console.log('Similar nightly failure issue already exists, skipping creation');
          }