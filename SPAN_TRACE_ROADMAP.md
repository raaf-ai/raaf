# RAAF Span & Trace Enhancement Roadmap

## Executive Summary

We've identified **40+ data points** across 6 categories that would significantly enhance observability, cost tracking, debugging, and quality metrics in RAAF.

**Total Implementation:** 3 phases over 2-3 quarters
**Priority Phases:** Phase 1 (immediate) + Phase 2 (next quarter)
**Phase 1 Timeline:** 3-4 weeks, ~20 hours effort
**Phase 1 Value:** ğŸ’°ğŸ’°ğŸ’° High (direct cost tracking impact)

---

## ğŸ“Š The 40+ Data Points by Category

### 1. Agent-Level Data (8 types)

| # | Data Type | Examples | Why Important | Priority |
|---|-----------|----------|---------------|----------|
| 1 | Input/Output Guardrails | Profanity, PII filtering | Security audit trail | ğŸŸ  Medium |
| 2 | Memory/Context Management | Context vars, growth rate | Memory optimization | ğŸŸ  Medium |
| 3 | Handoff Decisions | From/to agents, data size | Workflow tracing | ğŸŸ  Medium |
| 4 | Tool Execution Details | Duration, results, retries | Bottleneck identification | ğŸ”´ **HIGH** |
| 5 | Error Recovery Attempts | Retry count, backoff delays | System resilience | ğŸ”´ **HIGH** |
| 6 | Prompt Variations | Template used, variables | Audit trail | ğŸŸ  Medium |
| 7 | Schema Validation Results | Pass/fail, violations | Output quality | ğŸŸ  Medium |
| 8 | JSON Repair Operations | Repairs applied, success | LLM output quality | ğŸŸ¢ Low |

**Total:** 8 distinct agent-level data streams

---

### 2. LLM-Level Data (7 types) â­ HIGHEST VALUE

| # | Data Type | Examples | Why Important | Priority |
|---|-----------|----------|---------------|----------|
| 1 | Token Usage (Detailed) | Input, output, cache read | ğŸ’° Cost calculation | ğŸ”´ **HIGH** |
| 2 | Cost Calculations | Per-request, per-agent | ğŸ’° Budget tracking | ğŸ”´ **HIGH** |
| 3 | Latency/Timing | TTFT, per-token, network | âš¡ Performance SLAs | ğŸ”´ **HIGH** |
| 4 | Retry Attempts & Backoff | Count, errors, delays | ğŸ›¡ï¸ Reliability metrics | ğŸ”´ **HIGH** |
| 5 | Provider-Specific Metadata | Request ID, rate limits | ğŸ”— Multi-provider support | ğŸŸ  Medium |
| 6 | Model Capabilities Used | Tools, response format | ğŸ“Š Feature usage tracking | ğŸŸ  Medium |
| 7 | Rate Limiting Information | Remaining quota, reset time | ğŸ¯ Quota management | ğŸŸ  Medium |

**Total:** 7 distinct LLM data streams
**Phase 1 Impact:** ğŸ’°ğŸ’°ğŸ’° Direct cost visibility

---

### 3. Tool-Level Data (6 types)

| # | Data Type | Examples | Why Important | Priority |
|---|-----------|----------|---------------|----------|
| 1 | Tool Selection Reasoning | Available vs selected | ğŸ” Debug LLM decisions | ğŸŸ  Medium |
| 2 | Parameter Values & Validation | Args, validation errors | ğŸ” Catch LLM mistakes | ğŸŸ  Medium |
| 3 | Execution Duration | API call, processing time | âš¡ Bottleneck ID | ğŸ”´ **HIGH** |
| 4 | Error Handling | Error type, fallback | ğŸ›¡ï¸ Tool resilience | ğŸŸ  Medium |
| 5 | Result Size/Complexity | Bytes, nesting depth | ğŸ“Š Memory management | ğŸŸ¢ Low |
| 6 | Tool Chaining/Dependencies | Sequence, iterations | ğŸ”„ Workflow understanding | ğŸŸ¢ Low |

**Total:** 6 distinct tool data streams

---

### 4. Pipeline-Level Data (4 types)

| # | Data Type | Examples | Why Important | Priority |
|---|-----------|----------|---------------|----------|
| 1 | Agent Sequence/Order | Sequential vs parallel | ğŸ”„ Orchestration visibility | ğŸŸ  Medium |
| 2 | Data Transformations | Input/output per agent | ğŸ” Data flow tracing | ğŸŸ  Medium |
| 3 | Parallel Execution Metrics | Branches, sync wait time | âš¡ Parallelism efficiency | ğŸŸ  Medium |
| 4 | Pipeline Performance Metrics | Total duration, throughput | âš¡ SLA tracking | ğŸ”´ **HIGH** |

**Total:** 4 distinct pipeline data streams

---

### 5. Context/State Data (4 types)

| # | Data Type | Examples | Why Important | Priority |
|---|-----------|----------|---------------|----------|
| 1 | Context Variables @ Each Step | Keys, mutations | ğŸ” Debug context flow | ğŸŸ¢ Low |
| 2 | Context Size Evolution | Growth rate, max size | ğŸ“Š Memory prediction | ğŸŸ¢ Low |
| 3 | Memory Consumption | Process MB before/after | ğŸ“Š Resource planning | ğŸŸ¢ Low |
| 4 | Thread/Concurrency Context | Thread ID, pool size | ğŸ” Multi-threaded debugging | ğŸŸ¢ Low |

**Total:** 4 distinct context/state data streams

---

### 6. Quality Metrics (4 types)

| # | Data Type | Examples | Why Important | Priority |
|---|-----------|----------|---------------|----------|
| 1 | Response Quality Indicators | Score, hallucination | âœ¨ Output quality | ğŸŸ  Medium |
| 2 | Schema Validation Stats | Pass rate, violations | âœ¨ LLM consistency | ğŸŸ  Medium |
| 3 | Guardrail Violations | Violation rate, types | ğŸ”’ Compliance tracking | ğŸŸ  Medium |
| 4 | Fallback Usage | Type, success rate | ğŸ›¡ï¸ Recovery effectiveness | ğŸŸ  Medium |

**Total:** 4 distinct quality metric streams

---

## ğŸ¯ Phase 1: Immediate Priority (This Month)

### Phase 1 Scope: 6 Focus Areas

#### 1. Token Usage Tracking (6 data points)

```ruby
# Captures:
- Input tokens
- Output tokens
- Cache read tokens (new OpenAI feature!)
- Cache creation tokens (cost savings!)
- Total tokens
- Efficiency ratio (output/total)

# Value: Enables accurate cost calculation
# Effort: ~2 hours
# Files: LLM collector + UI component
```

**Sample Dashboard Display:**
```
LLM Usage Summary
â”œâ”€ Input: 1,250 tokens
â”œâ”€ Output: 342 tokens
â”œâ”€ Cache Hit: 500 tokens (saved!)
â”œâ”€ Total: 2,092 tokens
â””â”€ Efficiency: 16.3%
```

---

#### 2. Cost Calculation (6 data points)

```ruby
# Captures:
- Input cost ($0.0625 for example)
- Output cost ($0.0513 for example)
- Cache savings ($0.0020 for example)
- Total cost per request ($0.118)
- Cost per token
- Model + Provider tracking

# Value: Direct financial tracking per request
# Effort: ~3 hours (includes calculator class)
# Files: LLM collector + cost calculator + UI
```

**Sample Dashboard Display:**
```
Cost Breakdown
â”œâ”€ Input: $0.0625 (100K token rate: $5.00)
â”œâ”€ Output: $0.0513 (100K token rate: $15.00)
â”œâ”€ Cache Savings: -$0.0020 (4% reduction!)
â”œâ”€ Total: $0.1118 per request
â””â”€ Per Token: $0.00533
```

---

#### 3. Tool Execution Details (6 data points)

```ruby
# Captures:
- Tool name
- Input arguments
- Execution duration (ms)
- Result size (bytes)
- Success/failure
- Retry count

# Value: Identify bottlenecks, debug LLM tool selection
# Effort: ~3 hours
# Files: Tool collector + UI component
```

**Sample Dashboard Display:**
```
Tool Execution
â”œâ”€ Tool: web_search
â”œâ”€ Query: "BIM Netherlands"
â”œâ”€ Duration: 1,234 ms
â”œâ”€ Result Size: 2,345 bytes
â”œâ”€ Status: âœ… Success
â””â”€ Retries: 0
```

---

#### 4. Pipeline Performance Metrics (4 data points)

```ruby
# Captures:
- Total pipeline duration
- Per-agent durations
- Overhead time (coordination cost)
- Throughput (items/sec)

# Value: End-to-end visibility, SLA tracking
# Effort: ~2 hours
# Files: Pipeline span setup + UI
```

**Sample Dashboard Display:**
```
Pipeline Performance
â”œâ”€ Total: 46,234 ms
â”‚  â”œâ”€ MarketAnalyzer: 15,000 ms
â”‚  â”œâ”€ ProspectScorer: 30,000 ms
â”‚  â””â”€ Overhead: 1,234 ms
â”œâ”€ Throughput: 2.16 items/sec
â””â”€ Status: Within SLA âœ…
```

---

#### 5. Error Recovery Tracking (5 data points)

```ruby
# Captures:
- Retry attempt count
- Error per attempt
- Backoff delay applied
- Circuit breaker status
- Final success/failure

# Value: System resilience metrics, failure analysis
# Effort: ~3 hours
# Files: Error handling + collector
```

**Sample Dashboard Display:**
```
Error Recovery
â”œâ”€ Attempt 1: âŒ ConnectionTimeout â†’ Wait 1000ms
â”œâ”€ Attempt 2: âŒ RateLimited â†’ Wait 2000ms
â”œâ”€ Attempt 3: âœ… Success
â”œâ”€ Total Recovery Time: 3,000 ms
â””â”€ Status: Recovered (2 retries)
```

---

#### 6. Latency Metrics (3 data points)

```ruby
# Captures:
- Total LLM latency
- Time to first token (TTFT)
- Latency per token

# Value: Performance SLA compliance, perceived latency
# Effort: ~2 hours
# Files: LLM collector + UI
```

**Sample Dashboard Display:**
```
Latency Breakdown
â”œâ”€ Total: 2,345 ms
â”œâ”€ Time to First Token: 450 ms
â”œâ”€ Per Token: 6.8 ms
â””â”€ Network Latency: 150 ms
```

---

## Phase 1: Implementation Timeline

```
Week 1: Core Infrastructure
â”œâ”€ Create TokenCostCalculator class
â”œâ”€ Update LLM span collector (token capture)
â””â”€ Add utility helpers for cost calculation
  Effort: 6 hours

Week 2: Tool & Pipeline Integration
â”œâ”€ Update Tool span collector (execution details)
â”œâ”€ Setup Pipeline performance tracking
â”œâ”€ Add error recovery event recording
  Effort: 6 hours

Week 3: Rails UI Components
â”œâ”€ Create LLMConfig component with costs
â”œâ”€ Create Tool Execution section
â”œâ”€ Create Pipeline Performance dashboard
â”œâ”€ Add Error Recovery timeline
  Effort: 5 hours

Week 4: Testing & Documentation
â”œâ”€ Integration tests
â”œâ”€ UI validation
â”œâ”€ Documentation
â”œâ”€ Deployment
  Effort: 3 hours

Total: ~20 hours (2.5 engineering weeks)
```

---

## ğŸ’° Phase 1 Value Proposition

### Immediate Business Impact

**Cost Tracking:**
- âœ… Accurate per-request cost calculation
- âœ… Cost attribution by agent/tool
- âœ… Cache benefit tracking ($0.0020 per request = 4% savings!)
- âœ… Budget forecasting by workload

**Performance Insights:**
- âœ… TTFT monitoring for user experience
- âœ… Tool bottleneck identification
- âœ… Pipeline SLA tracking
- âœ… Throughput analysis

**Reliability:**
- âœ… Retry pattern analysis
- âœ… Error categorization
- âœ… Recovery effectiveness
- âœ… System stability metrics

### Example Impact Analysis

**For ProspectsRadar (estimated 1M API calls/month):**

```
Current State:
â”œâ”€ Estimated cost: ~$2,000/month (blind estimate)
â”œâ”€ No cost visibility per workflow
â””â”€ Performance issues unidentified

After Phase 1:
â”œâ”€ Exact cost: $1,850/month (with cache: -$75/month savings)
â”œâ”€ Cost per prospect scoring: $0.018
â”œâ”€ Bottleneck found: Tool latency consuming 60% of time
â”œâ”€ Tools optimized: -200ms per request = 15% speedup
â””â”€ Direct ROI: $900/year savings + better performance
```

---

## ğŸŸ  Phase 2: Medium Priority (Next Quarter)

### Phase 2 Scope: 6 Focus Areas

1. **Guardrail Tracking** (7 data points)
   - Security audit trail
   - Content filtering metrics
   - PII detection tracking

2. **Context Management** (4 data points)
   - Context flow visualization
   - Memory growth patterns
   - Variable mutations

3. **Handoff Tracking** (6 data points)
   - Agent chain visibility
   - Data transformation tracking
   - Handoff success metrics

4. **Schema Validation Metrics** (5 data points)
   - Pass rate tracking
   - Violation pattern analysis
   - Quality improvement trends

5. **Prompt Variation Tracking** (4 data points)
   - Template usage
   - Variable substitution
   - Prompt evolution

6. **Provider Analytics** (7 data points)
   - Multi-provider cost comparison
   - Rate limit tracking
   - API version management

**Phase 2 Effort:** ~25 hours
**Phase 2 Value:** ğŸ”’ Security audit trail, ğŸ“Š Quality improvements

---

## ğŸŸ¢ Phase 3: Advanced (Following Quarter)

### Phase 3 Scope: 6 Focus Areas

1. **Quality Metrics** (4 data points) - Response quality scoring
2. **Parallel Execution Analytics** (5 data points) - Parallelism ROI
3. **Memory Tracking** (5 data points) - Resource planning
4. **Concurrency Metrics** (4 data points) - Threading optimization
5. **Tool Chaining Analysis** (4 data points) - Workflow patterns
6. **Fallback Tracking** (4 data points) - Resilience analysis

**Phase 3 Effort:** ~20 hours
**Phase 3 Value:** âœ¨ Advanced analytics, ğŸ“Š Deep optimization

---

## ğŸ“‹ Implementation Checklist

### Phase 1: IMMEDIATE (3-4 weeks)

**Token Tracking**
- [ ] Add token capture to LLM collector
- [ ] Create TokenCostCalculator class
- [ ] Add UI section for token metrics
- [ ] Test with real agent executions

**Cost Calculation**
- [ ] Implement cost calculator for OpenAI
- [ ] Add cache benefit calculation
- [ ] Create cost attributes in spans
- [ ] Add cost display to UI

**Tool Execution**
- [ ] Capture tool execution events
- [ ] Add tool duration tracking
- [ ] Add result size tracking
- [ ] Create tool execution UI section

**Pipeline Performance**
- [ ] Add pipeline span tracking
- [ ] Capture per-agent durations
- [ ] Calculate critical path
- [ ] Create pipeline dashboard

**Error Recovery**
- [ ] Add retry event recording
- [ ] Capture backoff delays
- [ ] Track recovery success
- [ ] Create error recovery timeline UI

**Latency Metrics**
- [ ] Add latency captures
- [ ] Calculate TTFT if streaming
- [ ] Create latency dashboard
- [ ] Set latency alerts

**Documentation**
- [ ] Update API docs
- [ ] Create user guide
- [ ] Add examples
- [ ] Document cost calculation formula

---

## ğŸš€ Quick Start: Phase 1 Minimal Implementation

If you want to start immediately with **just the highest-value metrics**:

### Minimal Phase 1 (1 week, ~8 hours)

Focus on **tokens + costs + performance** only:

```ruby
# Step 1: LLM Collector (2 hours)
span usage_input_tokens: ->(comp) { comp.usage[:prompt_tokens] || 0 }
span usage_output_tokens: ->(comp) { comp.usage[:completion_tokens] || 0 }
span usage_total_tokens: ->(comp) { comp.usage[:total_tokens] || 0 }
span cost_total: ->(comp) { calc_cost(comp) }
span latency_ms: ->(comp) { comp.elapsed_time_ms || 0 }

# Step 2: UI Component (3 hours)
# Display tokens and cost in grid layout

# Step 3: Testing (2 hours)
# Verify data capture and display

# Step 4: Deploy (1 hour)
```

**Minimal Value:**
- ğŸ’° Cost visibility (token-based)
- âš¡ Latency tracking
- ğŸ“Š Basic performance metrics

---

## ğŸ“Š Comparison: Current vs Future State

```
CURRENT STATE (Today)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Agent Span View:
â”œâ”€ Agent name âœ“
â”œâ”€ Model âœ“
â”œâ”€ Status âœ“
â”œâ”€ Duration âœ“
â”œâ”€ Temperature âœ“ (NEW - just added!)
â””â”€ ... no visibility into cost, tokens, tools, errors

Questions You CANNOT Answer:
â”œâ”€ How much did this agent cost?
â”œâ”€ How many tokens did it use?
â”œâ”€ Which tool was slow?
â”œâ”€ Did it retry? How many times?
â”œâ”€ How long did it wait at each stage?
â””â”€ Is output quality improving?


FUTURE STATE (Phase 1 Complete)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Agent Span View:
â”œâ”€ Agent Configuration (name, model, settings)
â”œâ”€ Token Usage (input, output, cache, total)
â”œâ”€ Cost Breakdown (input cost, output cost, savings, total)
â”œâ”€ Performance Metrics (latency, TTFT, per-token)
â”œâ”€ Tool Execution (name, duration, result, status)
â”œâ”€ Error Recovery (retries, backoff, success)
â””â”€ Pipeline Timeline (sequential agent execution)

Questions You CAN Now Answer:
â”œâ”€ Cost per request: $0.118 âœ“
â”œâ”€ Total tokens used: 2,092 âœ“
â”œâ”€ Slowest tool: web_search @ 1,234ms âœ“
â”œâ”€ Retry pattern: 2 attempts, total 3sec delay âœ“
â”œâ”€ TTFT: 450ms (acceptable) âœ“
â””â”€ Quality trending: Pass rate 94.5% (improving) âœ“
```

---

## ğŸ¯ Success Metrics

### Phase 1 Success Criteria

- âœ… All token data captured for 100% of LLM calls
- âœ… Cost calculated within Â±2% of actual OpenAI invoices
- âœ… Latency metrics accurate to within Â±5%
- âœ… Tool execution details captured for 100% of tool calls
- âœ… Error recovery tracking captures 100% of retry events
- âœ… Pipeline performance metrics show <1% overhead
- âœ… UI dashboard loads in <2 seconds
- âœ… All data queryable via database
- âœ… Documentation complete and tested
- âœ… Team trained on new metrics

### Phase 1 Business KPIs

- ğŸ“Š Cost visibility: Â±2% accuracy to OpenAI invoice
- âš¡ Performance tracking: SLA baseline established
- ğŸ›¡ï¸ Reliability: Retry patterns identified
- ğŸ¯ Optimization targets: Bottlenecks identified (tools, latency)
- ğŸ’¡ Data quality: Baseline established for future improvements

---

## ğŸ”— Related Documents

For more details, see:

1. **COMPREHENSIVE_SPAN_TRACE_DATA.md**
   - Complete framework with all 40+ data points
   - Detailed implementation patterns
   - Code examples for each category

2. **SPAN_DATA_QUICK_REFERENCE.md**
   - Visual guide with examples
   - Priority matrix
   - Quick implementation checklist

3. **AGENT_SETTINGS_UI_DISPLAY.md**
   - Implementation guide for settings display
   - Already completed (Phase 0 âœ“)

4. **TEST_AGENT_SETTINGS_CAPTURE.md**
   - How to test the settings capture
   - Example usage patterns

---

## ğŸ“ Next Steps

### Immediate (This Week)
1. Review this roadmap
2. Discuss Phase 1 scope with team
3. Prioritize: Full Phase 1 vs Minimal Phase 1

### Short-term (This Month)
4. Allocate 2-3 engineering weeks for Phase 1
5. Begin token tracking implementation
6. Create cost calculator class
7. Update UI components

### Medium-term (Next Quarter)
8. Complete Phase 1 full rollout
9. Gather metrics and ROI analysis
10. Plan Phase 2 (guardrails, context, handoffs)

---

## ğŸ“„ Document Info

**File:** SPAN_TRACE_ROADMAP.md
**Version:** 1.0
**Created:** 2025-10-24
**Status:** Ready for Review
**Effort Estimate:** 65 hours total (20 + 25 + 20)
**Timeline:** 2-3 quarters
**Priority:** Phase 1 (immediate), Phase 2 (next quarter)

---

**Summary:** We have a clear, phased roadmap to add 40+ valuable data points to RAAF spans. Phase 1 (tokens, costs, performance) delivers immediate business value with 20 hours of effort. Phases 2-3 add security audit trail, quality metrics, and advanced analytics.

Ready to start Phase 1? Let's build complete observability! ğŸš€
