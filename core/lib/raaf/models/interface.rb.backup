# frozen_string_literal: true

require "securerandom"
require "logger"
require "net/http"
require_relative "../logging"
require_relative "../errors"

module RAAF

  module Models

    ##
    # Abstract base class for all model provider implementations
    #
    # This class defines the interface that all LLM providers must implement
    # to work with the RAAF framework. It provides:
    # - Standard method signatures for chat and streaming completions
    # - Common error handling for API responses
    # - Tool preparation and validation
    # - Model validation against supported models
    # - Universal handoff support via function calling
    # - Automatic Responses API compatibility
    #
    # Provider implementations handle the specifics of communicating with
    # different AI services (OpenAI, Anthropic, Groq, etc.) while presenting
    # a unified interface to the rest of the framework.
    #
    # @abstract Subclass and implement the required methods
    #
    # @example Implementing a custom provider
    #   class MyProvider < ModelInterface
    #     def chat_completion(messages:, model:, tools: nil, stream: false, **kwargs)
    #       # Implementation specific to your API
    #     end
    #
    #     def supported_models
    #       ["my-model-v1", "my-model-v2"]
    #     end
    #
    #     def provider_name
    #       "MyProvider"
    #     end
    #   end
    #
    class ModelInterface

      include Logger

      # Retry configuration constants
      DEFAULT_MAX_ATTEMPTS = 3
      DEFAULT_BASE_DELAY = 1.0 # seconds
      DEFAULT_MAX_DELAY = 30.0 # seconds
      DEFAULT_MULTIPLIER = 2.0
      DEFAULT_JITTER = 0.1 # 10% jitter

      # Common retryable exceptions
      RETRYABLE_EXCEPTIONS = [
        Errno::ECONNRESET,
        Errno::ECONNREFUSED,
        Errno::ETIMEDOUT,
        Net::ReadTimeout,
        Net::WriteTimeout,
        Net::OpenTimeout,
        Net::HTTPTooManyRequests,
        Net::HTTPServiceUnavailable,
        Net::HTTPGatewayTimeout
      ].freeze

      # HTTP status codes that should trigger retry
      RETRYABLE_STATUS_CODES = [408, 429, 500, 502, 503, 504].freeze

      attr_accessor :retry_config

      ##
      # Initialize a new model provider
      #
      # @param api_key [String, nil] API key for authentication
      # @param api_base [String, nil] Custom API base URL
      # @param options [Hash] Additional provider-specific options
      #
      # @example Initialize with API key
      #   provider = MyProvider.new(api_key: ENV['MY_API_KEY'])
      #
      # @example Initialize with custom endpoint
      #   provider = MyProvider.new(
      #     api_key: "key",
      #     api_base: "https://custom.api.com/v1"
      #   )
      #
      def initialize(api_key: nil, api_base: nil, **options)
        @api_key = api_key
        @api_base = api_base
        @options = options
        @retry_config = default_retry_config
      end

      ##
      # Execute a chat completion request
      #
      # @abstract Must be implemented by subclasses
      #
      # @param messages [Array<Hash>] Conversation messages with :role and :content
      # @param model [String] Model identifier (e.g., "gpt-4", "claude-3")
      # @param tools [Array<Hash>, nil] Tool definitions for function calling
      # @param stream [Boolean] Whether to stream the response
      # @param kwargs [Hash] Additional provider-specific parameters
      #
      # @return [Hash] Response in standardized format with choices, usage, etc.
      #
      # @raise [NotImplementedError] If called on base class
      # @raise [AuthenticationError] If API key is invalid
      # @raise [RateLimitError] If rate limit exceeded
      # @raise [APIError] For other API errors
      #
      def chat_completion(messages:, model:, tools: nil, stream: false, **kwargs)
        raise NotImplementedError, "Subclasses must implement chat_completion"
      end

      ##
      # Execute a streaming chat completion request
      #
      # @abstract Must be implemented by subclasses
      #
      # @param messages [Array<Hash>] Conversation messages
      # @param model [String] Model identifier
      # @param tools [Array<Hash>, nil] Tool definitions
      # @param block [Proc] Block to yield chunks to
      #
      # @yield [chunk] Yields response chunks as they arrive
      # @yieldparam chunk [Hash] Partial response data
      #
      # @raise [NotImplementedError] If called on base class
      #
      def stream_completion(messages:, model:, tools: nil, &block)
        raise NotImplementedError, "Subclasses must implement stream_completion"
      end

      ##
      # Get list of supported models for this provider
      #
      # @abstract Must be implemented by subclasses
      #
      # @return [Array<String>] List of supported model identifiers
      #
      # @example
      #   provider.supported_models
      #   # => ["gpt-4", "gpt-4-turbo", "gpt-3.5-turbo"]
      #
      def supported_models
        raise NotImplementedError, "Subclasses must implement supported_models"
      end

      ##
      # Get the human-readable provider name
      #
      # @abstract Must be implemented by subclasses
      #
      # @return [String] Provider name (e.g., "OpenAI", "Anthropic")
      #
      def provider_name
        raise NotImplementedError, "Subclasses must implement provider_name"
      end

      ##
      # Default implementation of responses_completion
      #
      # This method provides automatic Responses API compatibility for any
      # provider that implements chat_completion. It converts between the
      # different API formats to ensure universal handoff support.
      #
      # @example Basic usage with handoff tools
      #   response = provider.responses_completion(
      #     messages: [{ role: "user", content: "I need billing help" }],
      #     model: "gpt-4",
      #     tools: [{
      #       type: "function",
      #       name: "transfer_to_billing",
      #       function: {
      #         name: "transfer_to_billing",
      #         description: "Transfer to billing agent",
      #         parameters: { type: "object", properties: {} }
      #       }
      #     }]
      #   )
      #   # Returns: { output: [...], usage: {...}, model: "gpt-4" }
      #
      # @example With input items for conversation continuation
      #   response = provider.responses_completion(
      #     messages: [{ role: "user", content: "Hello" }],
      #     model: "gpt-4",
      #     input: [
      #       { type: "message", role: "user", content: "Follow up" },
      #       { type: "function_call_output", call_id: "call_123", output: "Success" }
      #     ]
      #   )
      #
      # @param messages [Array<Hash>] Conversation messages
      # @param model [String] Model identifier
      # @param tools [Array<Hash>, nil] Tool definitions
      # @param stream [Boolean] Whether to stream response
      # @param previous_response_id [String, nil] Previous response ID for continuation
      # @param input [Array<Hash>, nil] Input items for Responses API
      # @param **kwargs [Hash] Additional parameters
      #
      # @return [Hash] Response in Responses API format with :output, :usage, :model, :id
      #
      def responses_completion(messages:, model:, tools: nil, stream: false, previous_response_id: nil, input: nil,
                               **kwargs)
        log_debug("🔧 INTERFACE: Converting chat_completion to responses_completion",
                  provider: provider_name,
                  has_tools: !tools.nil?,
                  tools_count: tools&.size || 0)

        # Convert input items back to messages if needed
        actual_messages = if input&.any?
                            convert_input_to_messages(input, messages)
                          else
                            messages
                          end

        # Call the provider's chat_completion method
        response = chat_completion(
          messages: actual_messages,
          model: model,
          tools: tools,
          stream: stream,
          **kwargs
        )

        # Convert response to Responses API format
        convert_chat_to_responses_format(response)
      end

      ##
      # Check if provider supports handoffs
      #
      # By default, handoff support is available if the provider supports
      # function calling (i.e., accepts tools parameter).
      #
      # @example Checking handoff support
      #   provider = MyProvider.new
      #   if provider.supports_handoffs?
      #     puts "Handoffs are supported!"
      #   else
      #     puts "Use content-based handoff fallback"
      #   end
      #
      # @return [Boolean] True if handoffs are supported, false otherwise
      #
      def supports_handoffs?
        supports_function_calling?
      end

      ##
      # Check if provider supports function calling
      #
      # This method inspects the chat_completion method signature to determine
      # if it accepts a tools parameter, which indicates function calling support.
      #
      # @example Checking function calling support
      #   provider = MyProvider.new
      #   if provider.supports_function_calling?
      #     puts "Provider supports function calling"
      #   else
      #     puts "Provider is text-only"
      #   end
      #
      # @return [Boolean] True if function calling is supported, false otherwise
      #
      def supports_function_calling?
        method(:chat_completion).parameters.any? { |param| param[1] == :tools }
      end

      ##
      # Get provider capabilities
      #
      # Returns a comprehensive hash of provider capabilities including
      # API support, streaming, function calling, and handoff support.
      #
      # @example Getting provider capabilities
      #   provider = MyProvider.new
      #   caps = provider.capabilities
      #   puts "Responses API: #{caps[:responses_api]}"
      #   puts "Function calling: #{caps[:function_calling]}"
      #   puts "Handoffs: #{caps[:handoffs]}"
      #
      # @return [Hash] Capability flags with keys:
      #   - :responses_api - Whether provider supports Responses API
      #   - :chat_completion - Whether provider supports Chat Completions API
      #   - :streaming - Whether provider supports streaming
      #   - :function_calling - Whether provider supports function calling
      #   - :handoffs - Whether provider supports handoffs
      #
      def capabilities
        {
          responses_api: respond_to?(:responses_completion),
          chat_completion: respond_to?(:chat_completion),
          streaming: respond_to?(:stream_completion),
          function_calling: supports_function_calling?,
          handoffs: supports_handoffs?
        }
      end

      protected

      ##
      # Validate that a model is supported by this provider
      #
      # @param model [String] Model identifier to validate
      # @raise [ArgumentError] If model is not supported
      #
      def validate_model(model)
        return if supported_models.include?(model)

        raise ArgumentError, "Model '#{model}' not supported by #{provider_name}"
      end

      ##
      # Prepare tools for API submission
      #
      # This method converts various tool formats (FunctionTool, Hash, etc.)
      # into the standardized format expected by AI provider APIs.
      #
      # @param tools [Array<FunctionTool, Hash>, nil] Tools to prepare
      # @return [Array<Hash>, nil] Prepared tool definitions or nil
      #
      # @example Tool preparation
      #   tools = [
      #     FunctionTool.new(method(:search)),
      #     { type: "web_search" }
      #   ]
      #   prepared = prepare_tools(tools)
      #   # => [
      #   #   { type: "function", name: "search", function: {...} },
      #   #   { type: "web_search" }
      #   # ]
      #
      def prepare_tools(tools)
        return nil if tools.nil? || tools.empty?

        prepared = tools.map do |tool|
          case tool
          when Hash
            tool
          when FunctionTool
            tool_hash = tool.to_h
            # DEBUG: Log the tool definition being sent to OpenAI
            log_debug_tools("Preparing tool for OpenAI API",
                            tool_name: tool_hash.dig(:function, :name),
                            tool_type: tool_hash[:type],
                            has_parameters: !tool_hash.dig(:function, :parameters).nil?,
                            properties_count: tool_hash.dig(:function, :parameters, :properties)&.keys&.length || 0,
                            required_count: tool_hash.dig(:function, :parameters, :required)&.length || 0)

            # Enhanced logging for array parameters to debug "items" property
            tool_hash.dig(:function, :parameters, :properties)&.each do |prop_name, prop_def|
              next unless prop_def[:type] == "array"

              log_debug_tools("🔍 OPENAI API ARRAY PROPERTY DEBUG for #{prop_name}",
                              full_property_definition: prop_def.inspect,
                              has_items_property: prop_def.key?(:items),
                              items_value: prop_def[:items].inspect)
              log_error("🚨 FOUND THE BUG! Array property '#{prop_name}' has nil items - this will cause OpenAI API error!") if prop_def[:items].nil?
            end

            tool_hash
          else
            # Check if this is a Tools module class when available
            raise ArgumentError, "Invalid tool type: #{tool.class}" unless defined?(RAAF::Tools)

            if tool.is_a?(RAAF::Tools::WebSearchTool) ||
               tool.is_a?(RAAF::Tools::HostedFileSearchTool) ||
               tool.is_a?(RAAF::Tools::HostedComputerTool)
              tool.to_tool_definition
            else
              raise ArgumentError, "Invalid tool type: #{tool.class}"
            end

          end
        end

        # DEBUG: Log the final prepared tools
        log_debug_tools("Final tools prepared for OpenAI API",
                        tools_count: prepared.length,
                        tool_names: prepared.map { |t| t.dig(:function, :name) || t[:type] }.compact)

        prepared
      end

      ##
      # Handle API error responses
      #
      # This method provides standardized error handling across all providers,
      # converting HTTP error codes into appropriate exception types.
      #
      # @param response [Net::HTTPResponse] The error response
      # @param provider [String] Provider name for error messages
      #
      # @raise [AuthenticationError] For 401 errors
      # @raise [RateLimitError] For 429 errors
      # @raise [ServerError] For 5xx errors
      # @raise [APIError] For other errors
      #
      def handle_api_error(response, provider)
        case response.code.to_i
        when 401
          raise AuthenticationError, "Invalid API key for #{provider}"
        when 429
          raise RateLimitError, "Rate limit exceeded for #{provider}"
        when 500..599
          raise ServerError, "Server error from #{provider}: #{response.code}"
        else
          raise APIError, "API error from #{provider}: #{response.code} - #{response.body}"
        end
      end

      ##
      # Convert Responses API input items to messages
      #
      # This method converts Responses API input items back to the standard
      # message format that chat_completion expects. This enables seamless
      # conversation continuation across different API formats.
      #
      # @example Converting input items
      #   input = [
      #     { type: "message", role: "user", content: "Hello" },
      #     { type: "function_call_output", call_id: "call_123", output: "Success" }
      #   ]
      #   messages = convert_input_to_messages(input, base_messages)
      #   # Returns: [...base_messages, { role: "user", content: "Hello" },
      #   #           { role: "tool", tool_call_id: "call_123", content: "Success" }]
      #
      # @param input [Array<Hash>] Input items from Responses API
      # @param base_messages [Array<Hash>] Base conversation messages
      # @return [Array<Hash>] Combined messages in Chat Completions format
      #
      def convert_input_to_messages(input, base_messages)
        # Start with base messages
        messages = base_messages.dup

        # Process input items
        input.each do |item|
          case item[:type] || item["type"]
          when "message"
            messages << {
              role: item[:role] || item["role"],
              content: item[:content] || item["content"]
            }
          when "function_call_output"
            messages << {
              role: "tool",
              tool_call_id: item[:call_id] || item["call_id"],
              content: item[:output] || item["output"]
            }
          end
        end

        messages
      end

      ##
      # Convert Chat Completions response to Responses API format
      #
      # This method converts a Chat Completions API response to the Responses API
      # format, enabling universal handoff support across different provider types.
      #
      # @example Converting a chat completion response
      #   chat_response = {
      #     "choices" => [{
      #       "message" => {
      #         "role" => "assistant",
      #         "content" => "I'll help you",
      #         "tool_calls" => [{
      #           "id" => "call_123",
      #           "type" => "function",
      #           "function" => { "name" => "transfer_to_billing", "arguments" => "{}" }
      #         }]
      #       }
      #     }],
      #     "usage" => { "total_tokens" => 25 },
      #     "model" => "gpt-4"
      #   }
      #
      #   responses_format = convert_chat_to_responses_format(chat_response)
      #   # Returns: {
      #   #   output: [
      #   #     { type: "message", role: "assistant", content: "I'll help you" },
      #   #     { type: "function_call", id: "call_123", name: "transfer_to_billing", arguments: "{}" }
      #   #   ],
      #   #   usage: { "total_tokens" => 25 },
      #   #   model: "gpt-4",
      #   #   id: "generated-uuid"
      #   # }
      #
      # @param response [Hash] Chat Completions API response
      # @return [Hash] Responses API format with :output, :usage, :model, :id
      #
      def convert_chat_to_responses_format(response)
        choice = response.dig("choices", 0) || response.dig(:choices, 0)
        return { output: [] } unless choice

        message = choice["message"] || choice[:message]
        return { output: [] } unless message

        output = []

        # Add text content
        if message["content"] || message[:content]
          content = message["content"] || message[:content]
          output << {
            type: "message",
            role: "assistant",
            content: content
          }
        end

        # Add tool calls
        if message["tool_calls"] || message[:tool_calls]
          tool_calls = message["tool_calls"] || message[:tool_calls]
          tool_calls.each do |tool_call|
            output << {
              type: "function_call",
              id: tool_call["id"] || tool_call[:id],
              name: tool_call.dig("function", "name") || tool_call.dig(:function, :name),
              arguments: tool_call.dig("function", "arguments") || tool_call.dig(:function, :arguments)
            }
          end
        end

        # Return in Responses API format with all necessary fields
        {
          output: output,
          usage: response["usage"] || response[:usage],
          model: response["model"] || response[:model],
          id: response["id"] || response[:id] || SecureRandom.uuid
        }
      end

    end

    ##
    # Raised when API authentication fails
    class AuthenticationError < Error; end

    ##
    # Raised when API rate limits are exceeded
    class RateLimitError < Error; end

    ##
    # Raised for server-side errors (5xx status codes)
    class ServerError < Error; end

    ##
    # Raised for general API errors
    class APIError < Error; end

  end

end
